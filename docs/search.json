[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 262 Notes",
    "section": "",
    "text": "Course Goals for STAT 262\nSTAT 262 is an introduction to the basic probability theory that allows us to the use the statistical methods seen in courses like STAT 102 and STAT 212. By the end of the course, you will:\n\nUnderstand and be able to apply the basic rules of probability, including sample spaces, conditional probability, independence, and Bayes’ theorem\nUnderstand and be able to articulate the definition of random variable for a specific scenario\nRecognize and be able to use the different function associated with random variables: cumulative distribution function, probability density/mass function, moment generating function\nUse density/mass functions to find moments\nUse common discrete and continuous probability distributions to model real-world scenarios\nUnderstand and be able to use multivariate probability distributions, including marginal and conditional distributions, independence, and covariance between random variables\nDerive and use probability distributions of functions of random variables using the methods of transformations, Jacobians, and moment generating functions\nUnderstand and be able to recognize random samples, including random samples drawn from the normal distribution\nUnderstand and be able to recognize the distribution of functions of normal random variables\nUnderstand elementary convergence concepts, including the law of large numbers and the Central Limit Theorem",
    "crumbs": [
      "Course Goals for STAT 262"
    ]
  },
  {
    "objectID": "Section 1 Probability.html",
    "href": "Section 1 Probability.html",
    "title": "1  Probability",
    "section": "",
    "text": "1.1 Basic Probability Definitions and Calculations\nWhen we are uncertain about an outcome’s occurrence (e.g., whether a coin will come up heads or tails, the number of dots observed on the roll of a die, whether or not the bus will be late), we typically quantify this uncertainty with a probability. Probability is the foundation upon which all of statistics is built, and it a provides a framework for modeling populations, experiments, and almost anything that could be considered a random phenomenon.\nA sample space, denoted by \\(\\cal{S}\\), is comprised of all possible outcomes of a random phenomenon.\nAn event is a collection of possible outcomes. Each event \\(A\\) is a subset of \\(\\cal{S}\\).\nWe want to formalize the idea of the “chance” that event \\(A\\) occurs. We will do this by defining the probability of each \\(A\\), which we denote \\(P(A)\\).\nProbabilities are calculated by defining functions on sets, and should be defined for all possible events. One thing that must be true: \\[\n0 \\leq P(A) \\leq 1\n\\]\nMore formally, a probability function is defined as follows.\nGiven a sample space \\(\\cal{S}\\), a probability function is a function P(\\(\\cdot\\)) that satisfies\nRequirements (1) - (3) are called the\nAny function P(\\(\\cdot\\)) that satisfies the Axioms of Probability is called a probability function.\nHere’s a (hopefully) obvious theorem:\nTheorem: Let \\((\\cal{S}, \\hbox{P})\\) be a sample space and associated probability function, respectively. For any event \\(A \\in \\cal{S}\\)\nAnd another, maybe less obvious.\nTheorem: Let \\((\\cal{S}, \\hbox{P})\\) be a sample space and associated probability function, respectively. For any events \\(A\\), \\(B \\in \\cal{S}\\)\nWe’ll use these theorems often when calculating probabilities. However, we first need to figure out how to assign probabilities to specific events. In some cases, we can do that by figuring out how many possible events there are in a sample space.\nIn a finite sample space, when all outcomes are equally likely, the number of possible outcomes can be used to make probability assignments.\nExample: Toss a fair, six-sided die\nOther Examples:\nOften, it is difficult to list all of the outcomes in a sample space, even when it is finite. In such circumstances, other methods must be employed to count the number of outcomes in a sample space.\nExample:\nWhy is the number of possible outcomes important? Because when each outcome is equally likely, we use the number of possible outcomes to find the probabilities of various events. The problem is that Fundamental Theorem of Counting doesn’t always (obviously) work. To determine the number of ways a task can be completed, we often need to consider whether sampling occurs with or without replacement and whether sampling is ordered or unordered. We’re not going to go into a ton of detail here. Some of these combinatorics scenarios really aren’t helpful to us, and most situations don’t involve outcomes that are all equally likely.\nSome quick and dirty examples:\nOrdered, with replacement: Number of different 6-character license plates if the first 3 characters must be letters and the final 3 characters must be numbers.\nIn general:\nOrdered, without replacement: Number of different lead-off (i.e., first three batters) batting orders for a baseball team consisting of 9 players.\nIn general:\nUnordered, without replacement: A student is to answer 7 out of 10 questions on an exam. How many choice are there?\nIn general:\nUnordered, with replacement: This one is not terribly intuitive, and not terribly useful anyway. I couldn’t think of a realistic scenario. In case you care:\nExample: Suppose Alabama (which does not have a state lottery) is considering four possible lottery drawing configurations:\nBottom line: the total number of ways to pick \\(r\\) items from a total of \\(n\\) distinguishable items depends on whether or not order matters, as well as whether sampling is done with or without replacement.\nUnordered, without replacement will turn out to be the most case we use most often (and we’ll use that one A LOT).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "Section 1 Probability.html#basic-probability-definitions-and-calculations",
    "href": "Section 1 Probability.html#basic-probability-definitions-and-calculations",
    "title": "1  Probability",
    "section": "",
    "text": "If the sample space \\(\\cal{S}\\) consists of \\(n\\) possible outcomes, and these \\(n\\) outcomes are equally likely, the probability of any of these \\(\\{s_1,s_2,\\dots, s_n\\}\\) outcomes is\n\n\n\nIf the set \\(A\\) is some collection of these outcomes, then\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipFundamental Theorem of Counting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSix different cages, each with 40 balls numbered 1-40. Winners’ selections must be in the same sequence as the numbers drawn from the cages.\n\n\n\nOne cage with 40 balls numbered 1-40. Winners’ selections must be in the same sequence as the six numbers drawn from the cage.\n\n\n\nOne cage with 40 balls numbered 1-40 Winners’ selections must match the six numbers drawn from the cage.\n\n\n\nSix different cages, each with 40 balls numbered 1-40. Winners’ selections must match the numbers drawn from the cages.\n\n\n\nNumber of Ways to Pick \\(r\\) Items from \\(n\\) Distinguishable Objects\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\nOrder Matters\n\n\n\n\n\n\n\n\n\nOrder Doesn’t Matter",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "Section 1 Probability.html#conditional-probability",
    "href": "Section 1 Probability.html#conditional-probability",
    "title": "1  Probability",
    "section": "1.2 Conditional Probability",
    "text": "1.2 Conditional Probability\nLet’s look at the following table:\n\n\n\n\nSurvived\nDid Not Survive\n\n\n\n\nFirst Class\n201\n123\n\n\nSecond Class\n118\n166\n\n\nThird Class\n181\n528\n\n\n\nThe counts in the table are the number of Titanic passengers that fell into the each of the categories. From this table, we can calculate some probabilities. Let’s consider the outcomes First Class and Survived.\n\n\n\n\nOften, we have partial information about a certain phenomenon and wish to know how this affects the probabilities of outcomes of interest to us, if at all. For example, we might want to know the probability a randomly selected student is a sophomore, given that we know they are enrolled in STAT 262.\nExample: Toss a fair die. Let \\(A=\\{1\\}\\) and \\(B=\\{1,3,5\\}\\). What is the probability of throwing a 1 given that an odd number is thrown?\n\n\n\n\n\n\n\n\n\nTipDefinition: Conditional Probability\n\n\n\n\n\n\n\n\n\n\n\n\nExample, again:\n\n\n\nThis definition of conditional probability leads to:\n\n\n\n\n\n\n\n\n\n\nTipLaw of Total Probability\n\n\n\nIf \\(A_1,A_2,\\dots\\) is a collection of mutually exclusive (\\(A_i \\cap A_j = \\emptyset\\) for all \\(i \\neq j\\)) and exhaustive\n(\\(P(\\cup_{i=1}^{\\infty} A_i)=1\\)) events, and if \\(P(A_i) &gt; 0\\) for all \\(i\\), then for any event \\(B\\),\n\n\n\\(P(B)=\\)\n\n\n\n\n\n\n\n\nVenn Diagram:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipGeneral Form of Bayes’ Theorem\n\n\n\nSuppose \\(A_1,A_2,\\dots,\\) partition \\(\\cal{S}\\).\n\n\n\\(P(A_i|B) =\\)\n\n\n\n\n\n\n\n\n\n\nExample: Of travelers arriving at a small airport, 60% fly on major airlines, 30% fly on privately owned planes, and the remainder fly on commercially owned planes not belonging to a major airline. Of those traveling on major airlines, 50% are traveling for business reasons, whereas 60% of those arriving on private planes and 90% of those arriving on other commercially owned planes are traveling for business reasons.\nLet’s first construct a table.\n\n\n\n\n\n\n\n\n\n\nSuppose we randomly select one person arriving at this airport. What is the probability the person . . .\n\nis traveling on business?\n\n\n\n\n\n\n\n\nis traveling for business on a privately owned plane?\n\n\n\n\n\narrived on a privately owned plane, given the person is traveling on business?\n\n\n\n\n\nis traveling on business, given the person is flying on a commercially owned plane not belonging to a major airline?\n\nExample: A diagnostic test for a disease is such that it (correctly) detects the disease in 95% of the individuals who actually have the disease. Also, if a person does not have the disease, the test will report that he or she does not have it with probability 0.9. Only 1% of the population has the disease in question. If a person is chosen at random from the population and the diagnostic test indicates that she has the disease, what is the conditional probability that she does, in fact, have the disease?\n\n\n\nExample: A student answers a multiple choice exam question that offers four possible answers. Suppose the probability the student knows the answer to the question is 0.8 and the probability the student will guess is 0.2. If the student guesses, the probability of selecting the correct answer is 0.25. If the student correctly answers a question, what is the probability the student really knew the correct answer?\n\n\n\nConsider the following table:\n\nAll flights arriving at an airport on a single day\n\n\n\nLate\nOn Time\n\n\n\n\nDomestic\n12\n109\n\n\nInternational\n6\n53\n\n\n\nFind the probability that a randomly selected flight on this day was on time.\n\n\n\n\nFind the probability that a randomly selected flight was on time, given that it was a domestic flight.\n\n\n\n\nWhat do you notice about these two values?\n\n\nDoes this make sense in the context of this scenario? What do you think it means?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "Section 1 Probability.html#independence",
    "href": "Section 1 Probability.html#independence",
    "title": "1  Probability",
    "section": "1.3 Independence",
    "text": "1.3 Independence\nSometimes the occurrence of one event, \\(B\\), will have no effect on the probability of another event, \\(A\\). If \\(A\\) and \\(B\\) are unrelated, then intuitively it should be the case that\n\nAlso, it follows that\n\n\n\n\n\n\n\n\n\n\n\nTipDefinition: Statistical Independence\n\n\n\nTwo events, \\(A\\) and \\(B\\) are statistically independent if and only if\n\n\n\n\nExtending this to multiple events . . .\n\n\n\n\n\n\nTipDefinition: Mutual Independence\n\n\n\nA collection of events \\(A_1, A_2, \\dots A_n\\) are if and only if for any subcollection \\(A_{i_1},A_{i_2},\\dots, A_{i_k}\\)\n\n\n\n\n\n\n\n\nExample: A mouse caught in a maze has to maneuver through three successive escape hatches in order to escape. If the hatches operate independently and the probabilities for the mouse to successfully get through them are 0.6, 0.4, and 0.2, respectively, what are the probabilities that the mouse:\n\nwill be able to escape?\n\n\n\n\n\n\nwill not be able to escape?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "Section 2 Random Variables.html",
    "href": "Section 2 Random Variables.html",
    "title": "2  Random Variables and their Associated Functions",
    "section": "",
    "text": "2.1 Random Variables\nOften, we are interested in a numerical measurement of the outcome of a random experiment. For example, we might want to know the number of insects treated with a dose of a new insecticide that are killed. In this case, the outcome is the survival status of each `dosed’ insect, and the numerical measurement in which we are interested is the number that died. However, the observed number varies depending on the actual result of the experiment. This type of variable is called a .\nExample: Suppose we roll two dice. There are 36 possible outcomes:\n\\[\n\\cal{S} =  \\left \\{ \\begin{array}{llllll} 1,1 & 2,1 & 3,1 & 4,1 & 5,1 & 6,1 \\\\ 1,2 & 2,2 & 3,2 & 4,2 & 5,2 & 6,2 \\\\ 1,3 & 2,3 & 3,3 & 4,3 & 5,3 & 6,3 \\\\\n1,4 & 2,4 & 3,4 & 4,4 & 5,4 & 6,4 \\\\ 1,5 & 2,5 & 3,5 & 4,5 & 5,5 & 6,5 \\\\ 1,6 & 2,6 & 3,6 & 4,6 & 5,6 & 6,6 \\end{array} \\right \\} \\Rightarrow \\hbox{\\hspace{80mm}}\n\\]\nIn the game of SKUNK, we don’t necessarily care about the specific outcome of a roll. For example, we don’t care to know that, specifically, a (2,3) was rolled. Instead, what are we interested in?\nThe number of ones rolled varies among the 36 points in the sample space, and an observed value of the variable, \\(X\\), denoted by \\(x\\), depends on the outcome of a random experiment (a roll of two dice). This variable (the number of ones rolled on two dice) is referred to as a random variable.\nThe support of \\(X\\), or the set of possible values of \\(X\\), is:\nThe corresponding probabilities for each of the possible values of \\(X\\) are:\nIdentifying the type(s) of random variable(s) we are interested in helps us decide which methods and procedures are most appropriate for certain problems and answering specific questions. In general, there are two types of random variables:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random Variables and their Associated Functions</span>"
    ]
  },
  {
    "objectID": "Section 2 Random Variables.html#random-variables",
    "href": "Section 2 Random Variables.html#random-variables",
    "title": "2  Random Variables and their Associated Functions",
    "section": "",
    "text": "TipDefinition: Random Variable\n\n\n\nA random variable is a function that associates a real number with each element in the sample space. That is, a random variable is a function from a sample space, \\(\\cal{S}\\), into the real numbers, \\(\\mathbb{R}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscrete Random Variable: If the sample space of a random variable is finite or countably infinite, then the random variable is a discrete random variable.\n\n\n\nContinuous Random Variable: If the sample space of a random variable is uncountably infinite, then the random variable is a continuous random variable. A continuous random variable can take on any value in an interval",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random Variables and their Associated Functions</span>"
    ]
  },
  {
    "objectID": "Section 2 Random Variables.html#distribution-functions",
    "href": "Section 2 Random Variables.html#distribution-functions",
    "title": "2  Random Variables and their Associated Functions",
    "section": "2.2 Distribution Functions",
    "text": "2.2 Distribution Functions\nEach random variable, \\(X\\), is associated with a function called the cumulative distribution function (CDF) of \\(X\\).\n\n\n\n\n\n\nTipDefinition: Cumulative Distribution Function\n\n\n\nThe cumulative distribution function or CDF of a random variable \\(X\\), denoted by \\(F_X(x)\\), is defined as\n\n\n\n\n\n\n\n\nExample: Before, we found the probabilities for the possible values of \\(X=\\) number of ones rolled on two dice. What is the CDF of \\(X\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTheorem:\n\n\n\nThe function \\(F_X(x)\\) is a CDF if and only if the following three conditions hold:\n\n\n\n\n\n\n\n\n\n\n\n\nA discrete CDF increases only in jumps.\nExample: A teenager has passed the written driving exam, but still needs to pass the road exam. Suppose the probability of passing the road exam is \\(p\\). Let \\(X\\) be the number of attempts required to pass the road test, and assume attempts are independent.\n\n\n\n\n\n\n\n\nContinuous CDFs have no discontinuities.\nExample: Suppose \\(X\\sim\\) Uniform(0,1). For this distribution, the CDF is:\n\\(F_X(x) = \\left \\{ \\begin{array}{lll} 0 & \\hbox{if } x &lt; 0 \\\\ x & \\hbox{if } 0 \\leq x &lt; 1 \\\\ 1 & \\hbox{if } x \\geq 1 \\end{array} \\right .\\)\nFor a continuous CDF, \\(P(X=a)=0\\). Why?\n\n\n\nExample: Suppose the time a person must wait in line (in minutes) at airport security has cdf \\[\nF_X(x) = \\left \\{ \\begin{array}{ll} 0 & \\hbox{if } x &lt; 0 \\\\ 1-\\exp(-x/15) & \\hbox{if } 0 \\leq x &lt; \\infty \\end{array} \\right .\n\\]\n\nWhat is the probability a randomly selected passenger gets through the security line in less than 5 minutes?\n\n\n\nWhat is the probability a randomly selected passenger waits in the security line longer than 5 minutes?\n\n\n\nWhat is the probability a randomly selected passenger waits in line between 5 and 10 minutes?\n\n\n\n\n\n\n\n\n\n\n\n\nTipDefinition: Identically Distributed\n\n\n\nTwo random variables \\(X\\) and \\(Y\\) are identically distributed if they have the same distribution function. That is,",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random Variables and their Associated Functions</span>"
    ]
  },
  {
    "objectID": "Section 2 Random Variables.html#density-and-mass-functions",
    "href": "Section 2 Random Variables.html#density-and-mass-functions",
    "title": "2  Random Variables and their Associated Functions",
    "section": "2.3 Density and Mass Functions",
    "text": "2.3 Density and Mass Functions\nAssociated with any random variable \\(X\\) and its CDF \\(F_X(x)\\) is another function, called either the probability density function (pdf) or probability mass function (pmf). The terms pmf and pdf refer, respectively, to the discrete and continuous cases (though pdf often gets used for both types of RVs). Both pmfs and pdfs are concerned with “point probabilities” of random variables.\n\n\n\n\n\n\nTipDefinition: Probability Mass Function\n\n\n\nThe probability mass function or pmf of a discrete random variable \\(X\\), denoted by \\(f_X(x)\\), is given by\n\n\n\nLet \\(p_j\\) be the size of the jump in the CDF at \\(x=x_j\\), \\(j=1,2,\\dots\\), then\n\n\n\n\n\n\n\nExample: In game of SKUNK, we defined \\(X=\\) number of ones rolled on two dice. \\(X\\) has pmf:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTheorem:\n\n\n\nA function, \\(f_X(x)\\), is a pmf if and only if\n\n\n\n\n\n\n\n\n\nExample: A car dealer has 30 cars available for immediate sale, of which 10 are classified as compact cars. 3 customers arrive and buy cars. Define the random variable \\(N\\) to be the number of compact cars sold. What is the pmf for \\(N\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the probability exactly 2 compact cars are purchased?\n\n\n\n\nWhat is the probabilty less than 2 compact cars are purchased?\n\n\n\n\nThis shows the special relationship between CDFs and pmfs for discrete random variables:\n\n\n\nExample (cont’d): What is the CDF for \\(N\\)=number of compact cars sold?\n\n\nProbability density functions are associated with (absolutely) continuous random variables and (absolutely) continuous CDFs. A probability mass function (pmf) gives “point probabilities,” and pmfs may be summed to get the CDF of a discrete random variable. For continuous random variables, the probability of any point is zero (i.e., P\\((X=a)=0\\)). As a result, we define the probability density function (pdf) for a continuous random variable differently.\n\n\n\n\n\n\nTipRelationship between CDFs and pdfs\n\n\n\nThe relationship between the CDF and the probability density function (pdf) of a continuous random variable \\(X\\) is given by\n\n\n\nwhere \\(f_X(x)\\) is the pdf. By the Fundamental Theorem of Calculus,\n\n\n\n\n\n\nExample: Each day, mail arrives in the Statistics department office between 9 and 10 am. The mail is no more likely to arrive at one time than another. Let the random variable \\(X\\) represent the time (in hours) after 9 am that the mail arrives in the department office. Earlier, we determined \\(X\\sim\\) Uniform(0,1). For this distribution, the CDF is:\n\n\\(F_X(x) = \\left \\{ \\begin{array}{lll} 0 & \\hbox{if } x &lt; 0 \\\\ x & \\hbox{if } 0 \\leq x &lt; 1 \\\\ 1 & \\hbox{if } x \\geq 1 \\end{array} \\right .\\)\n\n\nFor this distribution, the pdf is:\n\n\n\n\n\n\n\nWhat is the probability the mail arrives before 9:15 am?\n\n\n\nThe pdf is a curve that describes the probability of observing \\(X\\) in some range of values, such as between \\(x_1\\) and \\(x_2\\). The probability is defined as:\n\n\n\nExample (con’t): Suppose that a random variable \\(Y\\sim\\) Uniform(\\(L,U\\)). In the previous example, \\(L=0\\) and \\(U=1\\). The pdf of \\(Y\\) is:\n\n\\(f_Y(y) = \\left \\{ \\begin{array}{lll} 0 & \\hbox{if } y &lt; L \\\\ c & \\hbox{if } L \\leq y \\leq U \\\\ 0 & \\hbox{if } y &gt; U \\end{array} \\right .\\)\n\n\nWhat should \\(c\\) be?\n\n\n\n\n\n\nTipTheorem:\n\n\n\nA function, \\(f_X(x)\\), is a pdf if and only if\n\n\n\n\n\n\n\n\n\nExample (con’t):\n\nWhat should \\(c\\) be?\n\n\n\n\n\n\n\nLet \\(Y\\) represent the time (in minutes) after 9 am that the mail arrives at the office. Assuming mail arrives between 9 and 10 am, and the mail is no more likely to arrive at one time than another, what is the pdf of \\(Y\\)?\n\n\n\n\n\nExample: Suppose the time between injury accidents in a nuclear power plant (in days) is a random variable has the pdf:\n\n\\(f_X(x) = \\left \\{ \\begin{array}{ll} \\frac{1}{3} e^{-x/3} &  x &gt; 0 \\\\ 0 & x \\leq 0 \\end{array} \\right .\\)\n\n\n\nVerify that this function is a pdf.\n\n\n\n\n\n\n\n\nWhat is the probability the time between accidents is between 2 and 5 days?\n\n\n\n\n\nWhat is the probability the time between 2 accidents is more than 365 days?\n\n\n\n\n\nFind the CDF of \\(X\\).\n\n\n\n\n\n\n\n\n\n\nHow could the CDF be used to find the probability that the time between 2 accidents is more than 365 days? Is this value easier to calculate using the CDF or pdf? Why do you think so?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random Variables and their Associated Functions</span>"
    ]
  }
]